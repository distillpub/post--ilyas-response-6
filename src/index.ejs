<!doctype html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <script src="https://distill.pub/template.v2.js"></script>
    <style>
        <%=require("raw-loader!../static/style.css") %>
    </style>
</head>

<body>

    <d-front-matter>
        <script type="text/json">{
  "title": "Learning from Incorrectly Labeled Data",
  "description": "Section 3.2 of Ilyas et al. (2019) shows that training a model on only adversarial errors leads to non-trivial generalization on the original test set. We show that these experiments are a specific case of learning from errors.",
  "password": "robustness",
  "authors": [
    {
      "author": "Eric Wallace",
      "authorURL": "http://www.ericswallace.com/",
      "affiliation": "UC Berkeley",
      "affiliationURL": "https://www.berkeley.edu/"
    }
  ],
  "katex": {
    "delimiters": [
      {
        "left": "$",
        "right": "$",
        "display": false
      },
      {
        "left": "$$",
        "right": "$$",
        "display": true
      }
    ]
  }
  }</script>
    </d-front-matter>

    <d-title></d-title>

    <d-article>

        <p>
            Section 3.2 of Ilyas et al. (2019) shows that training a model on only adversarial errors leads to
            non-trivial generalization on the original test set. We show that these experiments are a specific case of
            learning from errors. We start with a counterintuitive result--we take a completely mislabeled training set
            (without modifying the inputs) and use it to train a model that generalizes to the original test set. We
            then show that this result, and the results of Ilyas et al. (2019), are a special case of model
            distillation. In particular, since the incorrect labels are generated using a trained model, information
            about the trained model is being “leaked” into the dataset.
            We begin with the following question: what if we took the images in the training set (without any
            adversarial perturbations) and mislabeled them? Since the inputs are unmodified and mislabeled, intuition
            says that a model trained on this dataset should not generalize to the correctly-labeled test set.
            Nevertheless, we show that this intuition fails--a model <i>can</i> generalize.
            We first train a ResNet-18 on the CIFAR-10 training set for two epochs. The model reaches a training
            accuracy of 62.5% and a test accuracy of 63.1%. Next, we run the model on all of the 50,000 training data
            points and relabel them according to the model's predictions. Then, we filter out <i>all the correct
                predictions</i>. We are now left with an incorrectly labeled training set of size 18,768. We show four
            examples on the left of the Figure below:
        </p>

        <figure id="figure-1">
            <img src="images/image1.png">
            <figcaption>
                <a href="#figure-1" class="figure-number">1</a>
            </figcaption>
        </figure>

        <p>
            We then randomly initialize a new ResNet-18 and train it only on this mislabeled dataset. We train for 50
            epochs and reach an accuracy of 49.7% on the <i>original</i> test set. The new model has only ever seen
            incorrectly labeled, unperturbed images but can still non-trivially generalize.
        </p>

        <h2>This is Model Distillation Using Incorrect Predictions</h2>

        <p>
            How can this model and the models in Ilyas et al. (2019) generalize without seeing any correctly labeled
            data? Here, we show that since the incorrect labels are generated using a trained model, information is
            being “leaked” about that trained model into the mislabeled examples. In particular, this an indirect form
            of model distillation<d-cite key="hinton2015distilling"></d-cite>--training on this dataset allows a new
            model to somewhat recover the features of the original model.
        </p>

        <p>
            We first illustrate this distillation phenomenon using a two-dimensional problem. Then, we explore other
            peculiar forms of distillation for neural networks---we transfer knowledge despite the inputs being from
            another task.
        </p>

        <h3>Two-dimensional Illustration of Model Distillation</h3>

        <p>
            We construct a dataset of adversarial examples using a two-dimensional binary classification problem. We
            generate 32 random two-dimensional data points in $[0,1]^2$ and assign each point a random binary label. We
            then train a small feed-forward neural network on these examples, predicting 32/32 of the examples correctly
            (panel (a) in the Figure below).
        </p>

        <figure id="figure-2">
            <img src="images/image2.png">
            <figcaption>
                <a href="#figure-2" class="figure-number">2</a>
            </figcaption>
        </figure>

        <p>
            Next, we create adversarial examples for the original model using an $l_{\infty}$ ball of radius
            $\epsilon=0.12$. In panel (a) of the Figure above, we display the $\epsilon$-ball around each training
            point. In panel (b), we show the adversarial examples which cause the model to change its prediction (from
            correct to incorrect). We train a new feed-forward neural network on this dataset, resulting in the model in
            panel (c).
        </p>

        <p>
            Although this new model has never seen a correctly labeled example, it is able to perform non-trivially on
            the original dataset, predicting $23/32$ of the inputs correctly (panel (d) in the Figure). The new model's
            decision boundary loosely matches the original model's decision boundary, i.e., the original model has been
            somewhat distilled after training on its adversarial examples. This two-dimensional problem presents an
            illustrative version of the intriguing result that distillation can be performed using incorrect
            predictions.

        </p>

        <h3>
            Other Peculiar Forms of Distillation
        </h3>

        <p>
            Our experiments show that we can distill models using mislabeled examples. In what other peculiar ways can
            we learn about the original model? Can we use only <i>out-of-domain</i> data?
        </p>

        <p>
            We train a simple CNN model on MNIST, reaching 99.1% accuracy. We then run this model on the FashionMNIST
            training set and save its argmax predictions. The resulting dataset is nonsensical to humans—a "dress" is
            labeled as an "8".
        </p>

        <figure id="figure-3">
            <img src="images/image3.png">
            <figcaption>
                <a href="#figure-3" class="figure-number">3</a>
            </figcaption>
        </figure>

        <p>
            We then initialize a new CNN model and train it on this mislabeled FashionMNIST data. The resulting model
            reaches 91.04% accuracy on the MNIST test set. Furthermore, if we normalize the FashionMNIST images using
            the mean and variance statistics for MNIST, the model reaches 94.5% accuracy on the MNIST test set. This is
            another instance of recovering a functionally similar model to the original despite the new model only
            training on erroneous predictions.
        </p>

        <h3>
            Summary
        </h3>

        <p>
            These results show that training a model using mislabeled adversarial examples is a special case of learning
            from prediction errors. In other words, the perturbations added to adversarial examples in Section 3.2 of
            Ilyas et al. (2019) are not necessary to enable learning.
        </p>

    </d-article>



    <d-appendix>
        <h3>Backlink</h3>
        <p>
            Probablhy will go here? Maybe also on top?
        </p>

        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
    </d-appendix>

    <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
    <d-bibliography src="bibliography.bib"></d-bibliography>

</body>
